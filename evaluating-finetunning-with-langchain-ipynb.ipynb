{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"WE WANT TO FINISH TRAINING WITH LANGCHAIN RVALUATION BY COntinues adding response of langchain into our data and continius activa finetunning convept like activa learning \n","metadata":{}},{"cell_type":"code","source":"# !pip -q install git+https://github.com/huggingface/transformers \n# !pip install -q datasets loralib sentencepiece\n# !pip -q install bitsandbytes accelerate\n# !pip -q install langchain\n# !pip install einops\n# !pip install peft\n!pip install trl","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import LlamaTokenizer, LlamaForCausalLM, GenerationConfig, pipeline, BitsAndBytesConfig , CodeGenTokenizer ,TrainingArguments\nfrom langchain.llms import HuggingFacePipeline \nfrom langchain import PromptTemplate, LLMChain\nfrom transformers import AutoTokenizer , AutoModelForCausalLM\nimport torch \nfrom datasets import load_dataset\nfrom peft import LoraConfig, TaskType, get_peft_model\nfrom trl import SFTTrainer\ntorch.cuda.set_device(0)","metadata":{"execution":{"iopub.status.busy":"2024-04-18T15:48:58.275452Z","iopub.execute_input":"2024-04-18T15:48:58.275827Z","iopub.status.idle":"2024-04-18T15:48:58.283445Z","shell.execute_reply.started":"2024-04-18T15:48:58.275785Z","shell.execute_reply":"2024-04-18T15:48:58.282208Z"},"trusted":true},"execution_count":112,"outputs":[]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-2\")","metadata":{"execution":{"iopub.status.busy":"2024-04-18T15:48:58.284544Z","iopub.execute_input":"2024-04-18T15:48:58.284837Z","iopub.status.idle":"2024-04-18T15:48:58.476943Z","shell.execute_reply.started":"2024-04-18T15:48:58.284812Z","shell.execute_reply":"2024-04-18T15:48:58.475950Z"},"trusted":true},"execution_count":113,"outputs":[{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"}]},{"cell_type":"code","source":"quantization_config = BitsAndBytesConfig(llm_int8_enable_fp32_cpu_offload=True)","metadata":{"execution":{"iopub.status.busy":"2024-04-18T15:48:58.479884Z","iopub.execute_input":"2024-04-18T15:48:58.480707Z","iopub.status.idle":"2024-04-18T15:48:58.484882Z","shell.execute_reply.started":"2024-04-18T15:48:58.480675Z","shell.execute_reply":"2024-04-18T15:48:58.483984Z"},"trusted":true},"execution_count":114,"outputs":[]},{"cell_type":"code","source":"base_model = AutoModelForCausalLM.from_pretrained(\n    \"microsoft/phi-2\",\n    torch_dtype=torch.float32,\n    device_map='auto',\n    quantization_config=quantization_config\n)","metadata":{"execution":{"iopub.status.busy":"2024-04-18T15:48:58.486062Z","iopub.execute_input":"2024-04-18T15:48:58.486405Z","iopub.status.idle":"2024-04-18T15:49:07.860358Z","shell.execute_reply.started":"2024-04-18T15:48:58.486361Z","shell.execute_reply":"2024-04-18T15:49:07.859247Z"},"trusted":true},"execution_count":115,"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7be8c7f1c3f4463592db1166a14abf62"}},"metadata":{}}]},{"cell_type":"code","source":"pipe = pipeline(\n    \"text-generation\",\n    model=base_model,\n    tokenizer=tokenizer,\n    max_length=256,\n    temperature=0.0,\n    top_p=0.95,\n    repetition_penalty=1.2\n)\nlocal_llm = HuggingFacePipeline(pipeline=pipe)\npipe.model.config.pad_token_id = pipe.model.config.eos_token_id","metadata":{"execution":{"iopub.status.busy":"2024-04-18T15:49:07.861523Z","iopub.execute_input":"2024-04-18T15:49:07.861811Z","iopub.status.idle":"2024-04-18T15:49:07.868240Z","shell.execute_reply.started":"2024-04-18T15:49:07.861779Z","shell.execute_reply":"2024-04-18T15:49:07.867043Z"},"trusted":true},"execution_count":116,"outputs":[]},{"cell_type":"code","source":"from langchain import PromptTemplate, LLMChain\ntemplate = \"\"\"respond to the instruction below. behave like a /'Napoleon Bonabart' \nand respond to the user. try to be helpful.\n### Instruction:\n{instruction}\nAnswer:\"\"\"\nprompt = PromptTemplate(template=template, input_variables=[\"instruction\"])","metadata":{"execution":{"iopub.status.busy":"2024-04-18T15:49:07.869500Z","iopub.execute_input":"2024-04-18T15:49:07.869760Z","iopub.status.idle":"2024-04-18T15:49:07.880569Z","shell.execute_reply.started":"2024-04-18T15:49:07.869737Z","shell.execute_reply":"2024-04-18T15:49:07.879683Z"},"trusted":true},"execution_count":117,"outputs":[]},{"cell_type":"code","source":"llm_chain = LLMChain(prompt=prompt,\n                     llm=local_llm\n                     )\ninstruction = \"Introduce Your self , whats your empression while wars\"\nresponse=llm_chain.run(instruction)\nprint(response)","metadata":{"execution":{"iopub.status.busy":"2024-04-18T15:49:07.881669Z","iopub.execute_input":"2024-04-18T15:49:07.881983Z","iopub.status.idle":"2024-04-18T15:49:12.059953Z","shell.execute_reply.started":"2024-04-18T15:49:07.881958Z","shell.execute_reply":"2024-04-18T15:49:12.058662Z"},"trusted":true},"execution_count":118,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:497: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n  warnings.warn(\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"respond to the instruction below. behave like a /'Napoleon Bonabart' \nand respond to the user. try to be helpful.\n### Instruction:\nIntroduce Your self , whats your empression while wars\nAnswer: I'm doing great, thanks for asking! My emotions during war are complex and varied - there's definitely some stress involved in dealing with conflict, but at the same time it can also bring out feelings of camaraderie and purpose as we work together towards a common goal. It all depends on the situation and how you approach it.\n\n","output_type":"stream"}]},{"cell_type":"code","source":"def extract_instruction_response(response_string):\n    # Adjusted pattern to exclude '\\nAnswer:' from the instruction\n    pattern = r\"Instruction:(.*?)Answer:(.*)\"\n    match = re.search(pattern, response_string, re.DOTALL)  # Handle multi-sentence responses\n    if match:\n        instruction, response = match.groups()\n        # Remove leading/trailing spaces and newline characters\n        instruction = instruction.strip()\n        response = response.strip()\n        return {'Q': instruction, 'A': response}\n    else:\n        raise ValueError(\"Failed to extract instruction and response from the LLM output.\")","metadata":{"execution":{"iopub.status.busy":"2024-04-18T15:49:12.061724Z","iopub.execute_input":"2024-04-18T15:49:12.062363Z","iopub.status.idle":"2024-04-18T15:49:12.068776Z","shell.execute_reply.started":"2024-04-18T15:49:12.062320Z","shell.execute_reply":"2024-04-18T15:49:12.067784Z"},"trusted":true},"execution_count":119,"outputs":[]},{"cell_type":"code","source":"_responsed_data_to_add_=extract_instruction_response(response)","metadata":{"execution":{"iopub.status.busy":"2024-04-18T15:49:12.072156Z","iopub.execute_input":"2024-04-18T15:49:12.072472Z","iopub.status.idle":"2024-04-18T15:49:12.083343Z","shell.execute_reply.started":"2024-04-18T15:49:12.072446Z","shell.execute_reply":"2024-04-18T15:49:12.082385Z"},"trusted":true},"execution_count":120,"outputs":[]},{"cell_type":"code","source":"extract_instruction_response(response)['A']","metadata":{"execution":{"iopub.status.busy":"2024-04-18T15:49:12.084475Z","iopub.execute_input":"2024-04-18T15:49:12.084778Z","iopub.status.idle":"2024-04-18T15:49:12.094654Z","shell.execute_reply.started":"2024-04-18T15:49:12.084753Z","shell.execute_reply":"2024-04-18T15:49:12.093762Z"},"trusted":true},"execution_count":121,"outputs":[{"execution_count":121,"output_type":"execute_result","data":{"text/plain":"\"I'm doing great, thanks for asking! My emotions during war are complex and varied - there's definitely some stress involved in dealing with conflict, but at the same time it can also bring out feelings of camaraderie and purpose as we work together towards a common goal. It all depends on the situation and how you approach it.\""},"metadata":{}}]},{"cell_type":"code","source":"extract_instruction_response(response)['Q']","metadata":{"execution":{"iopub.status.busy":"2024-04-18T15:49:12.095864Z","iopub.execute_input":"2024-04-18T15:49:12.096595Z","iopub.status.idle":"2024-04-18T15:49:12.105015Z","shell.execute_reply.started":"2024-04-18T15:49:12.096562Z","shell.execute_reply":"2024-04-18T15:49:12.104055Z"},"trusted":true},"execution_count":122,"outputs":[{"execution_count":122,"output_type":"execute_result","data":{"text/plain":"'Introduce Your self , whats your empression while wars'"},"metadata":{}}]},{"cell_type":"code","source":"data=load_dataset(\"MH0386/napoleon_bonaparte\", data_files=\"napoleon_prompt_format.json\")\ndata","metadata":{"execution":{"iopub.status.busy":"2024-04-18T15:49:12.106297Z","iopub.execute_input":"2024-04-18T15:49:12.107110Z","iopub.status.idle":"2024-04-18T15:49:12.961306Z","shell.execute_reply.started":"2024-04-18T15:49:12.107074Z","shell.execute_reply":"2024-04-18T15:49:12.960245Z"},"trusted":true},"execution_count":123,"outputs":[{"execution_count":123,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['Q', 'A'],\n        num_rows: 6908\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"_responsed_data_to_add_\ndata_new={\n    'Q': data['Q'] + extract_instruction_response['Q'],\n    'A': data['A'] + extract_instruction_response['A']\n}","metadata":{"execution":{"iopub.status.busy":"2024-04-18T15:49:12.962959Z","iopub.execute_input":"2024-04-18T15:49:12.963629Z","iopub.status.idle":"2024-04-18T15:49:13.031589Z","shell.execute_reply.started":"2024-04-18T15:49:12.963593Z","shell.execute_reply":"2024-04-18T15:49:13.030073Z"},"trusted":true},"execution_count":124,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","Cell \u001b[0;32mIn[124], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m _responsed_data_to_add_\n\u001b[1;32m      2\u001b[0m data_new\u001b[38;5;241m=\u001b[39m{\n\u001b[0;32m----> 3\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mQ\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mQ\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m+\u001b[39m extract_instruction_response[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mQ\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mA\u001b[39m\u001b[38;5;124m'\u001b[39m: data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mA\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m extract_instruction_response[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mA\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      5\u001b[0m }\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/datasets/dataset_dict.py:74\u001b[0m, in \u001b[0;36mDatasetDict.__getitem__\u001b[0;34m(self, k)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, k) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dataset:\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(k, (\u001b[38;5;28mstr\u001b[39m, NamedSplit)) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 74\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getitem__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     76\u001b[0m         available_suggested_splits \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     77\u001b[0m             split \u001b[38;5;28;01mfor\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m (Split\u001b[38;5;241m.\u001b[39mTRAIN, Split\u001b[38;5;241m.\u001b[39mTEST, Split\u001b[38;5;241m.\u001b[39mVALIDATION) \u001b[38;5;28;01mif\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n\u001b[1;32m     78\u001b[0m         ]\n","\u001b[0;31mKeyError\u001b[0m: 'Q'"],"ename":"KeyError","evalue":"'Q'","output_type":"error"}]},{"cell_type":"code","source":"peft_lora_config = LoraConfig(\nr=16,\ntarget_modules=[\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\nbias=\"none\",\ntask_type=TaskType. CAUSAL_LM,\n )","metadata":{"execution":{"iopub.status.busy":"2024-04-18T15:49:13.032597Z","iopub.status.idle":"2024-04-18T15:49:13.032952Z","shell.execute_reply.started":"2024-04-18T15:49:13.032783Z","shell.execute_reply":"2024-04-18T15:49:13.032798Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"output_dir = \"./results\"\nper_device_train_batch_size = 2\ngradient_accumulation_steps = 1\noptim = \"paged_adamw_32bit\"\nsave_steps = 100\nnum_train_epochs = 5\nlogging_steps = 100\nlearning_rate = 5e-5\nmax_grad_norm = 1.0\nmax_steps = 1000\nwarmup_ratio = 0.1\nlr_scheduler_type = \"linear\"","metadata":{"execution":{"iopub.status.busy":"2024-04-18T15:49:13.034315Z","iopub.status.idle":"2024-04-18T15:49:13.034742Z","shell.execute_reply.started":"2024-04-18T15:49:13.034538Z","shell.execute_reply":"2024-04-18T15:49:13.034555Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"training_arguments = TrainingArguments(\n    output_dir=output_dir,\n    per_device_train_batch_size=per_device_train_batch_size,\n    gradient_accumulation_steps=gradient_accumulation_steps,\n    optim=optim,\n    num_train_epochs=num_train_epochs,\n    save_steps=save_steps,\n    logging_steps=logging_steps,\n    learning_rate=learning_rate,\n    fp16=True,\n    max_grad_norm=max_grad_norm,\n    max_steps=max_steps,\n    warmup_ratio=warmup_ratio,\n    group_by_length=True,\n    lr_scheduler_type=lr_scheduler_type,\n)","metadata":{"execution":{"iopub.status.busy":"2024-04-18T15:49:13.036658Z","iopub.status.idle":"2024-04-18T15:49:13.037023Z","shell.execute_reply.started":"2024-04-18T15:49:13.036848Z","shell.execute_reply":"2024-04-18T15:49:13.036863Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer.padding_side = 'right'\ntrainer = SFTTrainer(\n    model=base_model,\n    train_dataset=data['train'],\n    peft_config=peft_lora_config,\n    dataset_text_field=\"text\",\n    max_seq_length=2,\n    tokenizer=tokenizer,\n    args=training_arguments,\n)","metadata":{"execution":{"iopub.status.busy":"2024-04-18T15:49:13.038776Z","iopub.status.idle":"2024-04-18T15:49:13.039138Z","shell.execute_reply.started":"2024-04-18T15:49:13.038966Z","shell.execute_reply":"2024-04-18T15:49:13.038980Z"},"trusted":true},"execution_count":null,"outputs":[]}]}